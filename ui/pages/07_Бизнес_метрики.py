import json
import os
from pathlib import Path
from typing import Dict, Optional, Tuple

import joblib
import numpy as np
import pandas as pd
import requests
import streamlit as st

st.set_page_config(page_title="–ë–∏–∑–Ω–µ—Å‚Äë–º–µ—Ç—Ä–∏–∫–∏", layout="wide")
st.title("üíº –ë–∏–∑–Ω–µ—Å‚Äë–º–µ—Ç—Ä–∏–∫–∏: MAPE ‚Üí –¥–µ–Ω—å–≥–∏, SS/ROP, –≤–ª–∏—è–Ω–∏–µ")

RAW_DIR = Path(os.getenv("RAW_DIR", "data_raw"))
DW_DIR = Path(os.getenv("WAREHOUSE_DIR", "data_dw"))
MODELS_DIR = Path(os.getenv("MODELS_DIR", "models"))


@st.cache_data(show_spinner=False)
def load_train():
    p = RAW_DIR / "train.csv"
    if not p.exists():
        return None
    try:
        return pd.read_csv(p, parse_dates=["date"])  # –æ–∂–∏–¥–∞—é—Ç—Å—è date, store_nbr, family, sales
    except Exception:
        return None


@st.cache_data(show_spinner=False)
def load_metrics():
    p = DW_DIR / "metrics_per_sku.csv"
    if not p.exists():
        return None
    try:
        return pd.read_csv(p)
    except Exception:
        return None


@st.cache_data(show_spinner=False)
def load_prices():
    p = Path("configs/prices.csv")
    if not p.exists():
        return None
    try:
        df = pd.read_csv(p)
        # –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: family[, store_nbr, price, margin_rate, holding_cost]
        return df
    except Exception:
        return None


train = load_train()
metrics = load_metrics()
prices = load_prices()

if train is None:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω data_raw/train.csv ‚Äî –¥–ª—è —Ä–∞—Å—á—ë—Ç–æ–≤ –Ω—É–∂–µ–Ω –æ–±—ä—ë–º –ø—Ä–æ–¥–∞–∂.")
    st.stop()

# –°–ø–∏—Å–∫–∏ –≤—ã–±–æ—Ä–∞
pairs = (
    train[["store_nbr", "family"]]
    .dropna()
    .drop_duplicates()
    .sort_values(["store_nbr", "family"])  # type: ignore[arg-type]
    .values.tolist()
)

st.sidebar.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
store_sel = st.sidebar.selectbox("store_nbr", sorted(sorted(set(int(s) for s, _ in pairs))))
fam_opts = sorted([str(f) for s, f in pairs if int(s) == int(store_sel)])
family_sel = st.sidebar.selectbox("family", fam_opts)
valid_days = st.sidebar.slider("–î–Ω–µ–π –≤ —Ö–≤–æ—Å—Ç–µ (–¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö)", min_value=14, max_value=90, value=30, step=1)

# –ü–æ–¥—Å—Ç–∞–≤–∏–º –ø—Ä–∞–π—Å/–º–∞—Ä–∂—É/—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑ configs/prices.csv –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
def _defaults_from_prices(sto: int, fam: str):
    if prices is None or prices.empty:
        return None, None, None
    df = prices.copy()
    cand = df[df["family"].astype(str) == str(fam)]
    # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∑–∞–ø–∏—Å—å —Å –Ω—É–∂–Ω—ã–º store_nbr, –∑–∞—Ç–µ–º –±–µ–∑ store_nbr
    if "store_nbr" in df.columns:
        c2 = df[(df["family"].astype(str) == str(fam)) & (df["store_nbr"].astype(float) == float(sto))]
        if not c2.empty:
            cand = c2
    if cand.empty:
        return None, None, None
    row = cand.iloc[0].to_dict()
    return row.get("price"), row.get("margin_rate"), row.get("holding_cost")

price_def, margin_def, holding_def = _defaults_from_prices(int(store_sel), str(family_sel))

st.subheader("–í–≤–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
col1, col2, col3, col4, col5 = st.columns(5)
with col1:
    price = st.number_input("–¶–µ–Ω–∞", min_value=0.0, value=float(price_def or 3.5), step=0.1)
with col2:
    margin_rate = st.number_input("–ú–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å", min_value=0.0, max_value=1.0, value=float(margin_def or 0.25), step=0.01)
with col3:
    holding_cost = st.number_input("–•—Ä–∞–Ω–µ–Ω–∏–µ/–¥–µ–Ω—å", min_value=0.0, value=float(holding_def or 0.05), step=0.01)
with col4:
    lead_time_days = st.number_input("Lead time (–¥–Ω–µ–π)", min_value=1, value=2, step=1)
with col5:
    service_level = st.number_input("–£—Ä–æ–≤–µ–Ω—å —Å–µ—Ä–≤–∏—Å–∞", min_value=0.80, max_value=0.99, value=0.95, step=0.01)


def _z_from_service_level(p: float) -> float:
    table = {0.80: 0.8416, 0.90: 1.2816, 0.95: 1.6449, 0.975: 1.9600, 0.99: 2.3263}
    closest = min(table.keys(), key=lambda x: abs(x - p))
    return table[closest]


st.subheader("–†–∞—Å—á—ë—Ç –ø–æ –ø–∞—Ä–µ")
df_pair = train[(train["store_nbr"] == int(store_sel)) & (train["family"].astype(str) == str(family_sel))].copy()
df_pair = df_pair.sort_values("date")
tail = df_pair.tail(int(valid_days)).copy()
daily_mean = float(tail["sales"].mean()) if not tail.empty else 0.0

mape_pct = None
if metrics is not None and not metrics.empty:
    sub = metrics[(metrics["store_nbr"] == int(store_sel)) & (metrics["family"].astype(str) == str(family_sel))]
    if not sub.empty and "MAPE_%" in sub.columns:
        try:
            mape_pct = float(sub.iloc[0]["MAPE_%"])
        except Exception:
            mape_pct = None

if mape_pct is None:
    mape_pct = 20.0  # –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç

sigma = max((mape_pct / 100.0) * daily_mean, 0.0)
z = _z_from_service_level(float(service_level))
L = max(int(lead_time_days), 1)
safety_stock = z * sigma * (L ** 0.5)
rop = daily_mean * L + safety_stock

colA, colB, colC, colD = st.columns(4)
with colA:
    st.metric("–°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å/–¥–µ–Ω—å", f"{daily_mean:.3f}")
with colB:
    st.metric("MAPE %", f"{mape_pct:.2f}%")
with colC:
    st.metric("Safety Stock", f"{safety_stock:.1f}")
with colD:
    st.metric("Reorder Point", f"{rop:.1f}")

# –î–µ–Ω–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏: —É–ø—É—â–µ–Ω–Ω–∞—è –º–∞—Ä–∂–∞ (–Ω–µ–¥–æ–ø–æ—Å—Ç–∞–≤–∫–∞) –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ (–∏–∑–ª–∏—à–∫–∏)
expected_under_units = (1 - float(service_level)) * daily_mean * L
under_cost = expected_under_units * (float(margin_rate) * float(price))
over_cost = float(holding_cost) * float(safety_stock)

st.caption("–î–µ–Ω–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (–ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ)")
c1, c2 = st.columns(2)
with c1:
    st.metric("Underage (‚âà —É–ø—É—â–µ–Ω–Ω–∞—è –º–∞—Ä–∂–∞ –∑–∞ –æ–∫–Ω–æ –ø–æ—Å—Ç–∞–≤–∫–∏)", f"{under_cost:.2f}")
with c2:
    st.metric("Overage (‚âà —Ö—Ä–∞–Ω–µ–Ω–∏–µ/–¥–µ–Ω—å)", f"{over_cost:.2f}")

st.markdown("---")
st.subheader("MAPE ‚Üí –î–µ–Ω—å–≥–∏: –¥–Ω–µ–≤–Ω–æ–π –∏ –º–µ—Å—è—á–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç")
rev_loss_day = (mape_pct / 100.0) * daily_mean * float(price)
rev_loss_month = rev_loss_day * 30.0
margin_loss_month = rev_loss_month * float(margin_rate)

colE, colF, colG = st.columns(3)
with colE:
    st.metric("–ü–æ—Ç–µ—Ä—è –≤—ã—Ä—É—á–∫–∏/–¥–µ–Ω—å (–æ—Ü–µ–Ω–∫–∞)", f"{rev_loss_day:.2f}")
with colF:
    st.metric("–ü–æ—Ç–µ—Ä—è –≤—ã—Ä—É—á–∫–∏/–º–µ—Å (–æ—Ü–µ–Ω–∫–∞)", f"{rev_loss_month:.2f}")
with colG:
    st.metric("–ü–æ—Ç–µ—Ä—è –º–∞—Ä–∂–∏/–º–µ—Å (–æ—Ü–µ–Ω–∫–∞)", f"{margin_loss_month:.2f}")

# –†–∞—Å—á—ë—Ç SS/ROP —á–µ—Ä–µ–∑ API (FastAPI /reorder_point)
st.markdown("---")
st.subheader("–†–∞—Å—á—ë—Ç —á–µ—Ä–µ–∑ API (FastAPI)")

def _resolve_api_url() -> str:
    api = os.getenv("API_URL")
    if api:
        return api
    # –ü–æ–ø—Ä–æ–±—É–µ–º secrets.toml (–∫–∞–∫ –≤ –¥–∞—à–±–æ—Ä–¥–µ)
    candidates = [
        Path.home() / ".streamlit" / "secrets.toml",
        Path.cwd() / ".streamlit" / "secrets.toml",
        Path(__file__).resolve().parents[2] / ".streamlit" / "secrets.toml",
        Path(__file__).resolve().parent / ".streamlit" / "secrets.toml",
    ]
    for p in candidates:
        if p.exists():
            try:
                import tomllib
                data = tomllib.loads(p.read_text(encoding="utf-8"))
                return data.get("API_URL", "http://127.0.0.1:8000")
            except Exception:
                pass
    return "http://127.0.0.1:8000"

api_url = st.text_input("API URL", value=_resolve_api_url(), help="–ê–¥—Ä–µ—Å FastAPI —Å–µ—Ä–≤–∏—Å–∞")

def build_features_dict_for_pair(df_all: pd.DataFrame, s: int, f: str) -> Optional[Dict[str, float]]:
    # –í–æ–∑—å–º—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –ø–æ –ø–∞—Ä–µ –∏–∑ train –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏—á–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—è—Å—å –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ per‚ÄëSKU –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    try:
        from make_features import make_features as _mf
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å —Ñ–∏—á–∏ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –æ—Å—Ç–∞–ª—å–Ω—ã–º CSV)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º train —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è ‚Äî –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ —Ñ–∏—á–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        last_row = df_all[(df_all["store_nbr"] == int(s)) & (df_all["family"].astype(str) == str(f))].sort_values("date").tail(1)
        if last_row.empty:
            return None
        # –§allback: –∑–∞–≥—Ä—É–∑–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã, –∏ –ø–µ—Ä–µ—Å–æ–±–µ—Ä—ë–º —Ñ–∏—á–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        paths = {k: RAW_DIR / f"{k}.csv" for k in ["transactions", "oil", "holidays_events", "stores"]}
        trans = pd.read_csv(paths["transactions"], parse_dates=["date"]) if paths["transactions"].exists() else None
        oil = pd.read_csv(paths["oil"], parse_dates=["date"]) if paths["oil"].exists() else None
        hol = pd.read_csv(paths["holidays_events"], parse_dates=["date"]) if paths["holidays_events"].exists() else None
        stores = pd.read_csv(paths["stores"]) if paths["stores"].exists() else None
        Xf, _ = _mf(df_all, hol, trans, oil, stores, dropna_target=False)
        sub = Xf[(Xf["store_nbr"] == int(s)) & (Xf["family"].astype(str) == str(f))].sort_values("date")
        if sub.empty:
            return None
        # –°–ø–∏—Å–æ–∫ —Ñ–∏—á –∏–∑ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        base = f"{int(s)}__{str(f).replace(' ', '_')}"
        feat_json = MODELS_DIR / f"{base}.features.json"
        feat_cols = None
        if feat_json.exists():
            try:
                data = json.loads(feat_json.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    feat_cols = [c for c in data if c in sub.columns]
            except Exception:
                feat_cols = None
        if feat_cols is None:
            # –û–±—â–∏–π —Å–ø–∏—Å–æ–∫: –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö
            exclude = {"id", "sales", "date"}
            feat_cols = [c for c in sub.columns if (c not in exclude) and pd.api.types.is_numeric_dtype(sub[c])]
        last = sub.iloc[-1]
        feats = {}
        for name in feat_cols:
            val = last.get(name, 0.0)
            try:
                feats[name] = float(val)
            except Exception:
                feats[name] = 0.0
        return feats
    except Exception:
        return None

col_api1, col_api2 = st.columns(2)
with col_api1:
    if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —á–µ—Ä–µ–∑ API /reorder_point"):
        feats = build_features_dict_for_pair(train, int(store_sel), str(family_sel))
        if feats is None or not feats:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Ñ–∏—á–∏ –¥–ª—è –ø–∞—Ä—ã. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã (transactions/oil/holidays/stores).")
        else:
            payload = {
                "store_nbr": int(store_sel),
                "family": str(family_sel),
                "features": feats,
                "lead_time_days": int(lead_time_days),
                "service_level": float(service_level),
            }
            try:
                r = requests.post(f"{api_url}/reorder_point", json=payload, timeout=8)
                if not r.ok:
                    st.error(f"API –æ—à–∏–±–∫–∞: {r.status_code} {r.text}")
                else:
                    data = r.json()
                    st.success("SS/ROP –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ API")
                    c1, c2, c3, c4 = st.columns(4)
                    with c1: st.metric("Daily mean (API)", f"{data.get('daily_mean', 0):.3f}")
                    with c2: st.metric("Sigma (API)", f"{data.get('sigma_daily', 0):.3f}")
                    with c3: st.metric("Safety Stock (API)", f"{data.get('safety_stock', 0):.1f}")
                    with c4: st.metric("ROP (API)", f"{data.get('reorder_point', 0):.1f}")
            except Exception as e:
                st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ API: {e}")

# –ú–∞—Å—Å–æ–≤—ã–π –±–∏–∑–Ω–µ—Å‚Äë–æ—Ç—á—ë—Ç (–≤—ã–∑–æ–≤ scripts/business_impact_report.py)
st.markdown("---")
st.subheader("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–Ω—ã–π –±–∏–∑–Ω–µ—Å‚Äë–æ—Ç—á—ë—Ç (—Å–∫—Ä–∏–ø—Ç)")
colr1, colr2, colr3 = st.columns(3)
with colr1:
    tail_days = st.number_input("tail_days (—Å—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å)", min_value=7, max_value=90, value=30, step=1)
with colr2:
    valid_days_rep = st.number_input("valid_days (–Ω–∞–∏–≤–Ω—ã–π MAPE)", min_value=7, max_value=90, value=28, step=1)
with colr3:
    price_csv = st.text_input("prices.csv", value=str(Path("configs/prices.csv").resolve()))

run_rep = st.button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç (scripts/business_impact_report.py)")
if run_rep:
    import subprocess
    import sys
    cmd = [sys.executable, "scripts/business_impact_report.py",
           "--price_csv", price_csv,
           "--lead_time_days", str(int(lead_time_days)),
           "--service_level", str(float(service_level)),
           "--tail_days", str(int(tail_days)),
           "--valid_days", str(int(valid_days_rep)),
           "--out_csv", str(DW_DIR / "business_impact_report.csv")]
    try:
        res = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        if res.returncode != 0:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞: {res.stderr or res.stdout}")
        else:
            st.success("–û—Ç—á—ë—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: data_dw/business_impact_report.csv")
            if (DW_DIR / "business_impact_report.csv").exists():
                dfrep = pd.read_csv(DW_DIR / "business_impact_report.csv")
                st.dataframe(dfrep.head(200), use_container_width=True)
                st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç (CSV)", data=dfrep.to_csv(index=False).encode("utf-8"), file_name="business_impact_report.csv", mime="text/csv")
    except Exception as e:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫—Ä–∏–ø—Ç: {e}")

st.markdown("---")
st.subheader("–°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –≤–ª–∏—è–Ω–∏—é (–µ—Å–ª–∏ –µ—Å—Ç—å)")
impact_csv = DW_DIR / "business_impact_report.csv"
if impact_csv.exists():
    try:
        imp = pd.read_csv(impact_csv)
        # –ø—Ä–æ—Å—Ç—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        cf1, cf2 = st.columns(2)
        with cf1:
            sel_stores = st.multiselect("–ú–∞–≥–∞–∑–∏–Ω—ã", options=sorted(imp["store_nbr"].dropna().astype(int).unique().tolist()), default=[])
        with cf2:
            sel_fams = st.multiselect("–°–µ–º–µ–π—Å—Ç–≤–∞", options=sorted(imp["family"].dropna().astype(str).unique().tolist()), default=[])
        sub = imp.copy()
        if sel_stores:
            sub = sub[sub["store_nbr"].isin(sel_stores)]
        if sel_fams:
            sub = sub[sub["family"].astype(str).isin(sel_fams)]
        st.dataframe(sub, use_container_width=True)
        st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å impact CSV", data=sub.to_csv(index=False).encode("utf-8"), file_name="business_impact_report_filtered.csv", mime="text/csv")
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å business_impact_report.csv: {e}")
else:
    st.info("–§–∞–π–ª data_dw/business_impact_report.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ —á–µ—Ä–µ–∑ make impact –∏ configs/prices.csv.")

# –ë—ã—Å—Ç—Ä—ã–π –º–∞—Å—Å–æ–≤—ã–π —Ä–∞—Å—á—ë—Ç SS/ROP (–±–µ–∑ —Å–∫—Ä–∏–ø—Ç–∞)
st.markdown("---")
st.subheader("–ë—ã—Å—Ç—Ä—ã–π –º–∞—Å—Å–æ–≤—ã–π —Ä–∞—Å—á—ë—Ç SS/ROP (–±–µ–∑ —Å–∫—Ä–∏–ø—Ç–∞)")

# –ö–æ–Ω—Ç—Ä–æ–ª—ã
colm1, colm2, colm3, colm4 = st.columns(4)
with colm1:
    mass_tail_days = st.number_input("tail_days (—Å—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å)", min_value=7, max_value=90, value=30, step=1, key="mass_tail")
with colm2:
    sigma_method = st.radio("–ú–µ—Ç–æ–¥ œÉ", ["–ü–æ MAPE (–º–µ—Ç—Ä–∏–∫–∏)", "–ü–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º P50/P90 (–µ—Å–ª–∏ –µ—Å—Ç—å)"] , index=0)
with colm3:
    mass_max_pairs = st.number_input("–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–∞—Ä—ã", min_value=10, max_value=2000, value=200, step=10)
with colm4:
    mass_filters = st.checkbox("–§–∏–ª—å—Ç—Ä—ã –ø–æ –ø–∞—Ä–∞–º", value=False)

stores_all = sorted(train["store_nbr"].dropna().astype(int).unique().tolist())
fams_all = sorted(train["family"].dropna().astype(str).unique().tolist())
sel_stores = []
sel_fams = []
if mass_filters:
    cfs1, cfs2 = st.columns(2)
    with cfs1:
        sel_stores = st.multiselect("–ú–∞–≥–∞–∑–∏–Ω—ã", options=stores_all, default=[])
    with cfs2:
        sel_fams = st.multiselect("–°–µ–º–µ–π—Å—Ç–≤–∞", options=fams_all, default=[])

def _build_all_features():
    # –ì–æ—Ç–æ–≤–∏–º —Ñ–∏—á–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä (–Ω—É–∂–Ω–æ –¥–ª—è –º–µ—Ç–æ–¥–∞ –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º)
    try:
        paths = {k: RAW_DIR / f"{k}.csv" for k in ["transactions", "oil", "holidays_events", "stores"]}
        trans = pd.read_csv(paths["transactions"], parse_dates=["date"]) if paths["transactions"].exists() else None
        oil = pd.read_csv(paths["oil"], parse_dates=["date"]) if paths["oil"].exists() else None
        hol = pd.read_csv(paths["holidays_events"], parse_dates=["date"]) if paths["holidays_events"].exists() else None
        stores = pd.read_csv(paths["stores"]) if paths["stores"].exists() else None
        from make_features import make_features as _mf
        Xf, _ = _mf(train, hol, trans, oil, stores, dropna_target=False)
        return Xf
    except Exception:
        return None

def _sigma_from_quantiles(xrow: pd.Series, s: int, f: str) -> Tuple[Optional[float], Optional[float]]:
    base = f"{int(s)}__{str(f).replace(' ', '_')}"
    p50 = MODELS_DIR / f"{base}__q50.joblib"
    p90 = MODELS_DIR / f"{base}__q90.joblib"
    if (not p50.exists()) or (not p90.exists()):
        return None, None
    # —Å–ø–∏—Å–æ–∫ —Ñ–∏—á ‚Äî —Å–Ω–∞—á–∞–ª–∞ –∏–∑ features.json, –∏–Ω–∞—á–µ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ
    feat_cols = None
    fj = MODELS_DIR / f"{base}.features.json"
    if fj.exists():
        try:
            data = json.loads(fj.read_text(encoding="utf-8"))
            if isinstance(data, list):
                feat_cols = data
        except Exception:
            feat_cols = None
    if feat_cols is None:
        exclude = {"id", "sales", "date"}
        feat_cols = [c for c in xrow.index if (c not in exclude) and pd.api.types.is_numeric_dtype(type(xrow[c]))]
    try:
        mdl50 = joblib.load(p50)
        mdl90 = joblib.load(p90)
        X = np.array([[float(xrow.get(c, 0.0)) for c in feat_cols]], dtype=float)
        q50 = float(mdl50.predict(X)[0])
        q90 = float(mdl90.predict(X)[0])
        sigma = max((q90 - q50) / 1.2816, 0.0)
        return q50, sigma
    except Exception:
        return None, None

mass_btn = st.button("–ü–æ—Å—á–∏—Ç–∞—Ç—å SS/ROP –¥–ª—è –≤—Å–µ—Ö (–±—ã—Å—Ç—Ä–æ)")
if mass_btn:
    z = _z_from_service_level(float(service_level))
    # –ü—Ä–µ–¥–ø–æ–¥—Å—á—ë—Ç—ã
    Xall = _build_all_features() if sigma_method.startswith("–ü–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º") else None
    rows = []
    seen = 0
    for (s, f), dfp in train.groupby(["store_nbr", "family"], sort=False):
        if sel_stores and int(s) not in set(sel_stores):
            continue
        if sel_fams and str(f) not in set(sel_fams):
            continue
        seen += 1
        if seen > int(mass_max_pairs):
            break
        dfp = dfp.sort_values("date")
        tailp = dfp.tail(int(mass_tail_days))
        daily_mean_p = float(tailp["sales"].mean()) if not tailp.empty else 0.0
        sigma_p = None
        daily_from_q = None
        method_used = "mape"
        if sigma_method.startswith("–ü–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º") and Xall is not None:
            sub = Xall[(Xall["store_nbr"] == int(s)) & (Xall["family"].astype(str) == str(f))].sort_values("date")
            if not sub.empty:
                q50, sig = _sigma_from_quantiles(sub.iloc[-1], int(s), str(f))
                if (q50 is not None) and (sig is not None):
                    daily_from_q = q50
                    sigma_p = sig
                    method_used = "quantiles"
        if sigma_p is None:
            # –§–æ–ª–ª–±–µ–∫ –Ω–∞ MAPE
            mape_pct_p = None
            if metrics is not None and not metrics.empty and "MAPE_%" in metrics.columns:
                sel = metrics[(metrics["store_nbr"] == s) & (metrics["family"].astype(str) == str(f))]
                if not sel.empty:
                    try:
                        mape_pct_p = float(sel.iloc[0]["MAPE_%"])
                    except Exception:
                        mape_pct_p = None
            if mape_pct_p is None:
                mape_pct_p = 20.0
            sigma_p = max((mape_pct_p / 100.0) * daily_mean_p, 0.0)
        mean_used = daily_from_q if (daily_from_q is not None) else daily_mean_p
        ss = z * sigma_p * (int(lead_time_days) ** 0.5)
        rop_val = mean_used * int(lead_time_days) + ss
        rows.append({
            "store_nbr": int(s),
            "family": str(f),
            "daily_mean": mean_used,
            "sigma": sigma_p,
            "SS": ss,
            "ROP": rop_val,
            "method": method_used,
        })
    if rows:
        out = pd.DataFrame(rows)
        st.success(f"–†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø–∞—Ä: {len(out)} (–∏–∑ {seen} –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö)")
        st.dataframe(out, use_container_width=True)
        st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å SS/ROP (CSV)", data=out.to_csv(index=False).encode("utf-8"), file_name="mass_ss_rop.csv", mime="text/csv")
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ –Ω–∞–ª–∏—á–∏–µ –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π).")
