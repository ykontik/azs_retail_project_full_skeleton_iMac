import os
from pathlib import Path
from typing import List, Optional, Tuple

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st

from make_features import make_features

st.set_page_config(page_title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (—Å—Ç—Ä–æ–≥–æ–µ)", layout="wide")
st.title("üß™ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: LGBM per‚ÄëSKU vs Global CatBoost/XGB (—Å—Ç—Ä–æ–≥–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∏—á)")

RAW_DIR = Path(os.getenv("RAW_DIR", "data_raw"))
MODELS_DIR = Path(os.getenv("MODELS_DIR", "models"))
DW_DIR = Path(os.getenv("WAREHOUSE_DIR", "data_dw"))


@st.cache_data(show_spinner=False)
def load_raw():
    paths = {
        k: RAW_DIR / f"{k}.csv"
        for k in ["train", "transactions", "oil", "holidays_events", "stores"]
    }
    missing = [k for k, p in paths.items() if not p.exists()]
    if missing:
        return None, missing
    train = pd.read_csv(paths["train"], parse_dates=["date"])
    trans = pd.read_csv(paths["transactions"], parse_dates=["date"])
    oil = pd.read_csv(paths["oil"], parse_dates=["date"])
    hol = pd.read_csv(paths["holidays_events"], parse_dates=["date"])
    stores = pd.read_csv(paths["stores"])
    return (train, hol, trans, oil, stores), []


@st.cache_data(show_spinner=True)
def build_features():
    data, missing = load_raw()
    if data is None:
        return None, missing
    train, hol, trans, oil, stores = data
    Xfull, _ = make_features(train, hol, trans, oil, stores, dropna_target=False)
    # –±–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤
    for c in ["store_nbr", "family", "type", "city", "state", "cluster", "is_holiday"]:
        if c in Xfull.columns:
            try:
                Xfull[c] = Xfull[c].astype("category")
            except Exception:
                pass
    bool_cols = Xfull.select_dtypes(include=["bool"]).columns.tolist()
    if bool_cols:
        Xfull[bool_cols] = Xfull[bool_cols].astype("int8")
    return Xfull, []


def model_feature_names(model) -> Optional[List[str]]:
    names = getattr(model, "feature_name_", None)
    if names is None and hasattr(model, "booster_"):
        try:
            names = list(model.booster_.feature_name())
        except Exception:
            names = None
    if names is None and hasattr(model, "feature_names_in_"):
        try:
            names = list(model.feature_names_in_)
        except Exception:
            names = None
    if names is None and hasattr(model, "get_booster"):
        try:
            booster = model.get_booster()
            fn = getattr(booster, "feature_names", None)
            if fn:
                names = list(fn)
        except Exception:
            names = None
    return names


def load_meta_features() -> Tuple[Optional[List[str]], Optional[List[str]], Optional[List[str]]]:
    cb_feats = None
    cb_cats = None
    xgb_feats = None
    try:
        mcb = DW_DIR / "metrics_global_catboost.json"
        if mcb.exists():
            import json as _json

            data = _json.loads(mcb.read_text(encoding="utf-8"))
            cb_feats = data.get("features") if isinstance(data.get("features"), list) else None
            cats = data.get("categoricals")
            cb_cats = cats if isinstance(cats, list) else None
    except Exception:
        pass
    try:
        mxg = DW_DIR / "metrics_global_xgboost.json"
        if mxg.exists():
            import json as _json

            data = _json.loads(mxg.read_text(encoding="utf-8"))
            xgb_feats = data.get("features") if isinstance(data.get("features"), list) else None
    except Exception:
        pass
    return cb_feats, cb_cats, xgb_feats


def ensure_columns(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    X = df.copy()
    for c in cols:
        if c not in X.columns:
            X[c] = 0.0
    return X[cols]


def predict_catboost(
    model, df: pd.DataFrame, feature_list: List[str], categoricals: Optional[List[str]]
):
    from catboost import Pool

    X = ensure_columns(df, feature_list).copy()
    # CatBoost –ª—é–±–∏—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    if categoricals:
        for c in categoricals:
            if c in X.columns:
                try:
                    X[c] = X[c].astype(str)
                except Exception:
                    pass
        cat_idx = [i for i, c in enumerate(feature_list) if categoricals and c in set(categoricals)]
    else:
        cat_idx = []
    pool = Pool(X, feature_names=feature_list, cat_features=cat_idx if cat_idx else None)
    return model.predict(pool)


def predict_xgb(model, df: pd.DataFrame, feature_list: List[str]):
    # XGB –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –ø–æ–¥–∞–≤–∞—Ç—å –∫–∞–∫ DataFrame —Å –Ω—É–∂–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
    X = ensure_columns(df, feature_list).copy()
    # –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª–∞–º
    try:
        X = X.apply(pd.to_numeric, errors="coerce").fillna(0.0)
    except Exception:
        pass
    return model.predict(X)


def prepare_rf_features(df: pd.DataFrame, feature_list: List[str]) -> pd.DataFrame:
    X = ensure_columns(df, feature_list).copy()
    bool_cols = X.select_dtypes(include=["bool"]).columns.tolist()
    if bool_cols:
        X[bool_cols] = X[bool_cols].astype("int8")
    obj_cols = X.select_dtypes(include=["object", "string"]).columns.tolist()
    for col in obj_cols:
        X[col] = X[col].astype("category").cat.codes.astype("int16")
    cat_cols = X.select_dtypes(include=["category"]).columns.tolist()
    for col in cat_cols:
        X[col] = X[col].cat.codes.astype("int16")
    return X.fillna(0.0)


def predict_rf(model, df: pd.DataFrame, feature_list: List[str]):
    X = prepare_rf_features(df, feature_list)
    return model.predict(X)


# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö per‚ÄëSKU –º–æ–¥–µ–ª–µ–π (–±–µ—Ä—ë–º –±–∞–∑–æ–≤—ã–µ LGBM-—Ñ–∞–π–ª—ã –≤–∏–¥–∞ store__family.joblib)
models = sorted(
    [
        p
        for p in MODELS_DIR.glob("*.joblib")
        if "__q" not in p.name and not p.name.startswith("global_") and len(p.stem.split("__")) == 2
    ]
)
if not models:
    st.warning("–ù–µ—Ç per‚ÄëSKU –º–æ–¥–µ–ª–µ–π –≤ –∫–∞—Ç–∞–ª–æ–≥–µ models/. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.")
    st.stop()

pairs = []
for mp in models:
    try:
        s, f = mp.stem.split("__", 1)
        pairs.append((int(s), f.replace("_", " ")))
    except Exception:
        continue
pairs = sorted(set(pairs))

st.sidebar.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã")
store_sel = st.sidebar.selectbox("store_nbr", sorted(set(s for s, _ in pairs)))
fam_opts = sorted([f for s, f in pairs if s == store_sel])
family_sel = st.sidebar.selectbox("family", fam_opts)
back_days = st.sidebar.slider("–î–Ω–µ–π –≤ —Ö–≤–æ—Å—Ç–µ", min_value=28, max_value=180, value=90, step=7)
period = st.sidebar.radio("–ü–µ—Ä–∏–æ–¥", ["–î–µ–Ω—å", "–ù–µ–¥–µ–ª—è", "–ú–µ—Å—è—Ü"], index=0)

# –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª–∏ –ª–∏–Ω–∏–π ‚Äî –Ω–∞–¥ –≥—Ä–∞—Ñ–∏–∫–æ–º
st.subheader("–ü–æ–∫–∞–∑–∞—Ç—å –ª–∏–Ω–∏–∏")
col_t1, col_t2, col_t3, col_t4, col_t5, col_t6 = st.columns(6)
with col_t1:
    show_fact = st.checkbox("–§–∞–∫—Ç", value=True, key="t_fact_strict")
with col_t2:
    show_lgbm = st.checkbox("LGBM per‚ÄëSKU", value=True, key="t_lgbm_strict")
with col_t3:
    show_cb = st.checkbox("CatBoost (global)", value=True, key="t_cb_strict")
with col_t4:
    show_xgb = st.checkbox("XGB (global)", value=True, key="t_xgb_strict")
with col_t5:
    show_xgbps = st.checkbox("XGB per‚ÄëSKU", value=True, key="t_xgbps_strict")
with col_t6:
    show_rf = st.checkbox("RandomForest per‚ÄëSKU", value=True, key="t_rf_strict")

Xfull, missing = build_features()
if Xfull is None:
    st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö: " + ", ".join(missing))
    st.stop()

# –î–∞–Ω–Ω—ã–µ –ø–æ –ø–∞—Ä–µ
mask = (Xfull["store_nbr"] == int(store_sel)) & (Xfull["family"].astype(str) == str(family_sel))
df_pair = Xfull.loc[mask].sort_values("date").copy()
if df_pair.empty:
    st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–∞—Ä–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ data_raw/ –∏ make_features.")
    st.stop()

tail = df_pair.tail(int(back_days)).copy()
y_true = tail["sales"].values if "sales" in tail.columns else None

st.subheader("–û–ø—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
opt_col1, opt_col2, opt_col3, opt_col4 = st.columns(4)
with opt_col1:
    normalize_minmax = st.checkbox(
        "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä—è–¥—ã (min-max)",
        value=False,
        help="–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ 1 –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ö–≤–æ—Å—Ç–∞.",
        key="viz_norm_strict",
    )
with opt_col2:
    show_residuals = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç—å –æ—Å—Ç–∞—Ç–∫–∏ (y_true ‚àí y_pred)",
        value=False,
        disabled=y_true is None,
        key="viz_resid_strict",
    )
with opt_col3:
    separate_axes = st.checkbox(
        "–û—Ç–¥–µ–ª—å–Ω—ã–µ —à–∫–∞–ª—ã (–ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏)", value=False, key="viz_subplots_strict"
    )
with opt_col4:
    show_quantile_band = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç—å –∫–æ—Ä–∏–¥–æ—Ä P50‚ÄìP90 (–µ—Å–ª–∏ –µ—Å—Ç—å)",
        value=False,
        key="viz_quantiles_strict",
    )

# Per‚ÄëSKU LGBM
base_stem = f"{int(store_sel)}__{str(family_sel).replace(' ', '_')}"
lgb_path = MODELS_DIR / f"{base_stem}.joblib"
if not lgb_path.exists():
    st.error(f"Per-SKU –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {lgb_path.name}")
    st.stop()
mdl_lgb = joblib.load(lgb_path)
feat_lgb = model_feature_names(mdl_lgb)
if not feat_lgb:
    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–ø–∏—Å–æ–∫ —Ñ–∏—á per‚ÄëSKU –º–æ–¥–µ–ª–∏.")
    st.stop()

# Global CatBoost / XGB
cb_feats, cb_cats, xgb_feats = load_meta_features()
mdl_cb = None
cb_err = None
try:
    cb_path = MODELS_DIR / "global_catboost.cbm"
    if cb_path.exists():
        from catboost import CatBoostRegressor

        mdl_cb = CatBoostRegressor()
        mdl_cb.load_model(str(cb_path))
except Exception as e:
    mdl_cb = None
    cb_err = str(e)

xgb_global = None
xgb_err = None
try:
    xgb_path = MODELS_DIR / "global_xgboost.joblib"
    if xgb_path.exists():
        xgb_global = joblib.load(xgb_path)
except Exception as e:
    xgb_global = None
    xgb_err = str(e)

# XGB per‚ÄëSKU (–µ—Å–ª–∏ –µ—Å—Ç—å)
mdl_xgb_ps = None
ps_feats = None
try:
    xgbps_path = MODELS_DIR / f"{base_stem}__xgb.joblib"
    if xgbps_path.exists():
        mdl_xgb_ps = joblib.load(xgbps_path)
        ps_feats = getattr(mdl_xgb_ps, "feature_names_in_", None)
        if ps_feats is None:
            # fallback: –ø–æ–ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∏—á –∏–∑ .features.json —Ä—è–¥–æ–º —Å LGBM
            fj = MODELS_DIR / f"{base_stem}.features.json"
            if fj.exists():
                import json as _json

                data = _json.loads(fj.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    ps_feats = [c for c in data if c in tail.columns]
except Exception:
    mdl_xgb_ps = None

mdl_rf = None
rf_feats: Optional[List[str]] = None
rf_err: Optional[str] = None
try:
    rf_path = MODELS_DIR / f"{base_stem}__rf.joblib"
    if rf_path.exists():
        mdl_rf = joblib.load(rf_path)
        rf_feats = getattr(mdl_rf, "feature_names_in_", None)
        if rf_feats is None:
            rf_json = MODELS_DIR / f"{base_stem}__rf.features.json"
            if rf_json.exists():
                import json as _json

                data = _json.loads(rf_json.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    rf_feats = [c for c in data if isinstance(c, str)]
        if rf_feats is not None:
            rf_feats = list(dict.fromkeys(rf_feats))
except Exception as exc:
    mdl_rf = None
    rf_err = str(exc)


def agg_series(dates: pd.Series, y: np.ndarray, how: str):
    df = pd.DataFrame({"date": dates.values, "y": y})
    if how == "–î–µ–Ω—å":
        return df["date"].values, df["y"].values
    rule = "W" if how == "–ù–µ–¥–µ–ª—è" else "M"
    g = df.set_index("date").groupby(pd.Grouper(freq=rule)).sum().reset_index()
    return g["date"].values, g["y"].values


def agg_series_to_series(dates: pd.Series, y: np.ndarray | None, how: str) -> Optional[pd.Series]:
    if y is None:
        return None
    x_vals, y_vals = agg_series(dates, y, how)
    if len(x_vals) == 0:
        return pd.Series(dtype=float)
    index = pd.to_datetime(x_vals)
    return pd.Series(y_vals, index=index)


# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
X_tail_lgb = ensure_columns(tail, feat_lgb)
y_lgb = mdl_lgb.predict(X_tail_lgb)

y_cb = None
err_cb_pred = None
if mdl_cb is not None and cb_feats:
    try:
        y_cb = predict_catboost(mdl_cb, tail, cb_feats, cb_cats)
    except Exception as e:
        err_cb_pred = str(e)

y_xgb = None
err_xgb_pred = None
if xgb_global is not None and xgb_feats:
    try:
        y_xgb = predict_xgb(xgb_global, tail, xgb_feats)
    except Exception as e:
        err_xgb_pred = str(e)

y_xgbps = None
err_xgbps_pred = None
if mdl_xgb_ps is not None:
    try:
        feats = list(ps_feats) if ps_feats else feat_lgb
        y_xgbps = predict_xgb(mdl_xgb_ps, tail, feats)
    except Exception as e:
        err_xgbps_pred = str(e)

y_rf = None
err_rf_pred = None
if mdl_rf is not None:
    try:
        feats = list(rf_feats) if rf_feats else feat_lgb
        y_rf = predict_rf(mdl_rf, tail, feats)
    except Exception as e:
        err_rf_pred = str(e)

q50_series = None
q90_series = None
quantile_note: Optional[str] = None
if show_quantile_band:
    try:
        q50_path = MODELS_DIR / f"{base_stem}__q50.joblib"
        q90_path = MODELS_DIR / f"{base_stem}__q90.joblib"
        if q50_path.exists() and q90_path.exists():
            mdl_q50 = joblib.load(q50_path)
            mdl_q90 = joblib.load(q90_path)
            q50_raw = mdl_q50.predict(X_tail_lgb)
            q90_raw = mdl_q90.predict(X_tail_lgb)
            q50_series = agg_series_to_series(tail["date"], q50_raw, period)
            q90_series = agg_series_to_series(tail["date"], q90_raw, period)
        else:
            quantile_note = "–ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (P50/P90) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–∞—Ä—ã."
    except Exception as exc:
        quantile_note = f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {exc}"


# –ì—Ä–∞—Ñ–∏–∫
colors = {
    "–§–∞–∫—Ç": "#1f77b4",
    "LGBM": "#ff7f0e",
    "CatBoost (global)": "#2ca02c",
    "XGBoost (global)": "#d62728",
    "XGBoost (per‚ÄëSKU)": "#17becf",
    "RandomForest (per‚ÄëSKU)": "#9467bd",
}


def add_series_entry(
    entries: List[dict],
    label: str,
    values: Optional[np.ndarray],
    color: str,
    kind: str,
) -> None:
    series = agg_series_to_series(tail["date"], values, period)
    if series is None or series.empty:
        return
    entries.append({"label": label, "series": series, "color": color, "kind": kind})


series_entries: List[dict] = []
actual_series = agg_series_to_series(tail["date"], y_true, period)
if show_fact and actual_series is not None:
    series_entries.append(
        {"label": "–§–∞–∫—Ç", "series": actual_series, "color": colors["–§–∞–∫—Ç"], "kind": "actual"}
    )

if show_lgbm:
    add_series_entry(series_entries, "LGBM", y_lgb, colors["LGBM"], "pred")
if show_cb and (y_cb is not None):
    add_series_entry(series_entries, "CatBoost (global)", y_cb, colors["CatBoost (global)"], "pred")
if show_xgb and (y_xgb is not None):
    add_series_entry(series_entries, "XGBoost (global)", y_xgb, colors["XGBoost (global)"], "pred")
if show_xgbps and (y_xgbps is not None):
    add_series_entry(
        series_entries, "XGBoost (per‚ÄëSKU)", y_xgbps, colors["XGBoost (per‚ÄëSKU)"], "pred"
    )
if show_rf and (y_rf is not None):
    add_series_entry(
        series_entries, "RandomForest (per‚ÄëSKU)", y_rf, colors["RandomForest (per‚ÄëSKU)"], "pred"
    )

all_value_arrays: List[np.ndarray] = []
for entry in series_entries:
    if not entry["series"].empty:
        all_value_arrays.append(entry["series"].dropna().values)
if show_quantile_band and q50_series is not None and q90_series is not None:
    all_value_arrays.append(q50_series.dropna().values)
    all_value_arrays.append(q90_series.dropna().values)

if all_value_arrays:
    concat_values = np.concatenate([arr for arr in all_value_arrays if arr.size > 0])
    if concat_values.size > 0:
        global_min = float(np.nanmin(concat_values))
        global_max = float(np.nanmax(concat_values))
    else:
        global_min, global_max = 0.0, 1.0
else:
    global_min, global_max = 0.0, 1.0
global_range = global_max - global_min


def normalize_series(series: pd.Series) -> pd.Series:
    if not normalize_minmax or global_range <= 0:
        return series
    values = series.values
    norm = (values - global_min) / global_range if global_range > 0 else np.zeros_like(values)
    return pd.Series(norm, index=series.index)


plot_entries: List[dict] = []
for entry in series_entries:
    series = entry["series"]
    if series.empty:
        continue
    plot_entries.append(
        {
            **entry,
            "plot_series": normalize_series(series),
        }
    )

quantile_plot_pair: Optional[tuple[pd.Series, pd.Series]] = None
if show_quantile_band:
    if q50_series is not None and q90_series is not None:
        quantile_plot_pair = (normalize_series(q50_series), normalize_series(q90_series))
    elif quantile_note:
        st.info(quantile_note)

if not plot_entries:
    st.warning("–ù–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í–∫–ª—é—á–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —Å–µ—Ä–∏—é.")
else:
    ylabel = "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏" if normalize_minmax else "–ü—Ä–æ–¥–∞–∂–∏"
    title = f"–•–≤–æ—Å—Ç {int(back_days)} –¥–Ω., –ø–µ—Ä–∏–æ–¥: {period.lower()}"

    if separate_axes and len(plot_entries) > 0:
        fig, axes = plt.subplots(
            len(plot_entries),
            1,
            sharex=True,
            figsize=(18, max(3.0 * len(plot_entries), 6.0)),
        )
        if len(plot_entries) == 1:
            axes = [axes]
        for idx, (ax, entry) in enumerate(zip(axes, plot_entries)):
            series = entry["plot_series"]
            ax.plot(
                series.index,
                series.values,
                color=entry["color"],
                linewidth=2.0,
                label=entry["label"],
            )
            ax.set_ylabel(ylabel)
            ax.set_title(entry["label"], loc="left")
            ax.grid(True, alpha=0.3)
            if idx == len(plot_entries) - 1:
                ax.set_xlabel("–î–∞—Ç–∞/–ü–µ—Ä–∏–æ–¥")
        target_ax = axes[0]
        if quantile_plot_pair is not None:
            q_df = pd.concat(quantile_plot_pair, axis=1).dropna()
            q_df.columns = ["P50", "P90"]
            base_index = plot_entries[0]["plot_series"].index
            q_df = q_df.reindex(base_index).dropna()
            if not q_df.empty:
                target_ax.fill_between(
                    q_df.index,
                    q_df["P50"].values,
                    q_df["P90"].values,
                    color="#ffd166",
                    alpha=0.25,
                    label="P50‚ÄìP90",
                )
                target_ax.legend(loc="upper right")
        axes[0].set_title(title)
        fig.tight_layout()
        st.pyplot(fig, clear_figure=True)
        plt.close(fig)
    else:
        fig, ax = plt.subplots(figsize=(18, 7))
        for entry in plot_entries:
            series = entry["plot_series"]
            ax.plot(
                series.index,
                series.values,
                label=entry["label"],
                color=entry["color"],
                linewidth=2.0 if entry["kind"] == "actual" else 1.7,
            )
        if quantile_plot_pair is not None:
            q_df = pd.concat(quantile_plot_pair, axis=1).dropna()
            q_df.columns = ["P50", "P90"]
            base_index = plot_entries[0]["plot_series"].index
            q_df = q_df.reindex(base_index).dropna()
            if not q_df.empty:
                ax.fill_between(
                    q_df.index,
                    q_df["P50"].values,
                    q_df["P90"].values,
                    color="#ffd166",
                    alpha=0.25,
                    label="P50‚ÄìP90",
                )
        ax.set_title(title)
        ax.set_xlabel("–î–∞—Ç–∞/–ü–µ—Ä–∏–æ–¥")
        ax.set_ylabel(ylabel)
        ax.grid(True, alpha=0.3)
        if ax.lines:
            ax.legend(loc="upper right")
        fig.tight_layout()
        st.pyplot(fig, clear_figure=True)
        plt.close(fig)

residual_entries: List[dict] = []
if show_residuals and actual_series is not None:
    for entry in series_entries:
        if entry["kind"] != "pred":
            continue
        aligned_actual, aligned_pred = actual_series.align(entry["series"], join="inner")
        if aligned_actual.empty or aligned_pred.empty:
            continue
        residual_series = aligned_actual - aligned_pred
        residual_entries.append(
            {"label": entry["label"], "series": residual_series, "color": entry["color"]}
        )

if residual_entries:
    fig_res, ax_res = plt.subplots(figsize=(18, 4))
    for entry in residual_entries:
        ax_res.plot(
            entry["series"].index,
            entry["series"].values,
            label=entry["label"],
            color=entry["color"],
            linewidth=1.6,
        )
    ax_res.axhline(0.0, color="#666666", linewidth=1.0, linestyle="--")
    ax_res.set_title("–û—Å—Ç–∞—Ç–∫–∏ (y_true ‚àí y_pred)")
    ax_res.set_xlabel("–î–∞—Ç–∞/–ü–µ—Ä–∏–æ–¥")
    ax_res.set_ylabel("–ü—Ä–æ–¥–∞–∂–∏")
    ax_res.grid(True, alpha=0.3)
    ax_res.legend(loc="upper right")
    fig_res.tight_layout()
    st.pyplot(fig_res, clear_figure=True)
    plt.close(fig_res)


with st.expander("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", expanded=False):
    st.write(
        {
            "pair": {"store_nbr": int(store_sel), "family": family_sel},
            "tail_len": len(tail),
            "tail_dates": {
                "min": str(tail["date"].min()) if "date" in tail.columns else None,
                "max": str(tail["date"].max()) if "date" in tail.columns else None,
            },
            "lgbm": {"feat_count": len(feat_lgb) if feat_lgb else 0},
            "catboost": {
                "available": mdl_cb is not None,
                "predicted": y_cb is not None,
                "features_from_meta": bool(cb_feats),
                "errors": cb_err or err_cb_pred,
            },
            "xgb_global": {
                "available": xgb_global is not None,
                "predicted": y_xgb is not None,
                "features_from_meta": bool(xgb_feats),
                "errors": xgb_err or err_xgb_pred,
            },
            "xgb_per_sku": {
                "available": mdl_xgb_ps is not None,
                "predicted": y_xgbps is not None,
                "errors": err_xgbps_pred,
            },
            "rf_per_sku": {
                "available": mdl_rf is not None,
                "predicted": y_rf is not None,
                "errors": rf_err or err_rf_pred,
            },
        }
    )

# –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ö–≤–æ—Å—Ç—É (–¥–Ω–µ–≤–Ω—ã–µ)
st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ö–≤–æ—Å—Ç—É (–¥–Ω–µ–≤–Ω—ã–µ)")


def _mae_mape(y_true_arr, y_pred_arr):
    if y_true_arr is None or y_pred_arr is None:
        return None, None
    if len(y_true_arr) != len(y_pred_arr) or len(y_true_arr) == 0:
        return None, None
    mae = float(np.mean(np.abs(y_true_arr - y_pred_arr)))
    denom = np.where(y_true_arr == 0, 1, y_true_arr)
    mape = float(np.mean(np.abs((y_true_arr - y_pred_arr) / denom)) * 100.0)
    return mae, mape


col_m1, col_m2, col_m3, col_m4, col_m5 = st.columns(5)
mae_l, mape_l = _mae_mape(y_true, y_lgb)
if mae_l is not None:
    with col_m1:
        st.metric("LGBM ‚Äî MAE", f"{mae_l:.2f}")
        st.metric("LGBM ‚Äî MAPE %", f"{mape_l:.2f}%")
mae_cb, mape_cb = _mae_mape(y_true, y_cb)
if mae_cb is not None:
    with col_m2:
        st.metric("CatBoost ‚Äî MAE", f"{mae_cb:.2f}")
        st.metric("CatBoost ‚Äî MAPE %", f"{mape_cb:.2f}%")
mae_xgb, mape_xgb = _mae_mape(y_true, y_xgb)
if mae_xgb is not None:
    with col_m3:
        st.metric("XGBoost ‚Äî MAE", f"{mae_xgb:.2f}")
        st.metric("XGBoost ‚Äî MAPE %", f"{mape_xgb:.2f}%")
mae_xps, mape_xps = _mae_mape(y_true, y_xgbps)
if mae_xps is not None:
    with col_m4:
        st.metric("XGB per‚ÄëSKU ‚Äî MAE", f"{mae_xps:.2f}")
        st.metric("XGB per‚ÄëSKU ‚Äî MAPE %", f"{mape_xps:.2f}%")
mae_rf, mape_rf = _mae_mape(y_true, y_rf)
if mae_rf is not None:
    with col_m5:
        st.metric("RandomForest ‚Äî MAE", f"{mae_rf:.2f}")
        st.metric("RandomForest ‚Äî MAPE %", f"{mape_rf:.2f}%")

st.markdown("---")
st.subheader("–°–≤–æ–¥–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≤—ã–≥—Ä—É–∑–∫–∞")
metric_choice = st.radio("–ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (heatmap)", ["MAE", "MAPE"], horizontal=True)
dim_choice = st.radio("–†–∞–∑—Ä–µ–∑ Heatmap", ["–ü–æ –º–∞–≥–∞–∑–∏–Ω–∞–º", "–ü–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º"], horizontal=True)
max_pairs = st.slider(
    "–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞", min_value=10, max_value=300, value=100, step=10
)

# –§–∏–ª—å—Ç—Ä—ã –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º –∏ —Å—É–º–º–µ –ø—Ä–æ–¥–∞–∂ —Ö–≤–æ—Å—Ç–∞ (–∫–∞–∫ –≤ 03-–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ)
all_fams = sorted(set(f for _, f in pairs))
fam_filter = st.multiselect(
    "–§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º (–¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã)", options=all_fams, default=[]
)
min_tail_sales = st.number_input(
    "–ú–∏–Ω. —Å—É–º–º–∞ –ø—Ä–æ–¥–∞–∂ –≤ —Ö–≤–æ—Å—Ç–µ (–¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –ø–∞—Ä—ã)", min_value=0, value=0, step=10
)

pairs_all = pairs[:max_pairs]
if fam_filter:
    pairs_all = [p for p in pairs_all if p[1] in fam_filter]
rows = []
for s, f in pairs_all:
    try:
        mp = MODELS_DIR / f"{int(s)}__{str(f).replace(' ', '_')}.joblib"
        if not mp.exists():
            continue
        m = joblib.load(mp)
        feats = model_feature_names(m)
        df_p = (
            Xfull[(Xfull["store_nbr"] == int(s)) & (Xfull["family"] == f)]
            .sort_values("date")
            .tail(int(back_days))
            .copy()
        )
        if df_p.empty or not feats:
            continue
        for ff in feats:
            if ff not in df_p.columns:
                df_p[ff] = 0.0
        y_t = df_p["sales"].values
        tail_sum = float(np.sum(y_t))
        if tail_sum < float(min_tail_sales):
            continue
        y_l = m.predict(df_p[feats])
        row = {"store_nbr": int(s), "family": f}
        # CatBoost
        y_c = None
        if mdl_cb is not None and cb_feats:
            try:
                y_c = predict_catboost(mdl_cb, df_p, cb_feats, cb_cats)
            except Exception:
                y_c = None
        # XGB global
        y_x = None
        if xgb_global is not None and xgb_feats:
            try:
                y_x = predict_xgb(xgb_global, df_p, xgb_feats)
            except Exception:
                y_x = None
        # XGB per‚ÄëSKU
        y_xps = None
        xgbps_path = MODELS_DIR / f"{int(s)}__{str(f).replace(' ', '_')}__xgb.joblib"
        if xgbps_path.exists():
            try:
                mx = joblib.load(xgbps_path)
                fx = getattr(mx, "feature_names_in_", None) or feats
                for ff in fx:
                    if ff not in df_p.columns:
                        df_p[ff] = 0.0
                y_xps = predict_xgb(mx, df_p, list(fx))
            except Exception:
                y_xps = None
        # RandomForest per‚ÄëSKU
        y_rf_pair = None
        rf_path = MODELS_DIR / f"{int(s)}__{str(f).replace(' ', '_')}__rf.joblib"
        if rf_path.exists():
            try:
                mr = joblib.load(rf_path)
                fr = getattr(mr, "feature_names_in_", None)
                if fr is None:
                    rf_json = MODELS_DIR / f"{int(s)}__{str(f).replace(' ', '_')}__rf.features.json"
                    if rf_json.exists():
                        import json as _json

                        data = _json.loads(rf_json.read_text(encoding="utf-8"))
                        if isinstance(data, list):
                            fr = [c for c in data if isinstance(c, str)]
                fr_list = list(fr) if fr else feats
                y_rf_pair = predict_rf(mr, df_p, fr_list)
            except Exception:
                y_rf_pair = None

        # –º–µ—Ç—Ä–∏–∫–∏
        denom = np.where(y_t == 0, 1, y_t)
        mae_l = float(np.mean(np.abs(y_t - y_l)))
        mape_l = float(np.mean(np.abs((y_t - y_l) / denom)) * 100.0)
        wmape_l = float(np.sum(np.abs(y_t - y_l)) / max(tail_sum, 1.0) * 100.0)
        # Weekly MAPE –ø–æ –Ω–µ–¥–µ–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        dfw = pd.DataFrame({"date": df_p["date"].values, "y_true": y_t, "y_pred": y_l})
        gw = dfw.set_index("date").groupby(pd.Grouper(freq="W")).sum().reset_index()
        denom_w = np.where(gw["y_true"].values == 0, 1, gw["y_true"].values)
        weekly_mape_l = float(
            np.mean(np.abs((gw["y_true"].values - gw["y_pred"].values) / denom_w)) * 100.0
        )
        row.update(
            {
                "LGBM_MAE": round(mae_l, 2),
                "LGBM_MAPE": round(mape_l, 2),
                "LGBM_wMAPE": round(wmape_l, 2),
                "LGBM_wkMAPE": round(weekly_mape_l, 2),
            }
        )
        if y_c is not None:
            mae_c = float(np.mean(np.abs(y_t - y_c)))
            mape_c = float(np.mean(np.abs((y_t - y_c) / denom)) * 100.0)
            wmape_c = float(np.sum(np.abs(y_t - y_c)) / max(tail_sum, 1.0) * 100.0)
            # weekly –¥–ª—è CB
            dfw_c = pd.DataFrame({"date": df_p["date"].values, "y_true": y_t, "y_pred": y_c})
            gw_c = dfw_c.set_index("date").groupby(pd.Grouper(freq="W")).sum().reset_index()
            denom_w_c = np.where(gw_c["y_true"].values == 0, 1, gw_c["y_true"].values)
            weekly_mape_c = float(
                np.mean(np.abs((gw_c["y_true"].values - gw_c["y_pred"].values) / denom_w_c)) * 100.0
            )
            row.update(
                {
                    "CB_MAE": round(mae_c, 2),
                    "CB_MAPE": round(mape_c, 2),
                    "GAIN_CB_vs_LGBM_MAE": round(mae_l - mae_c, 2),
                    "GAIN_CB_vs_LGBM_MAPE": round(mape_l - mape_c, 2),
                    "CB_wMAPE": round(wmape_c, 2),
                    "CB_wkMAPE": round(weekly_mape_c, 2),
                }
            )
        if y_x is not None:
            mae_x = float(np.mean(np.abs(y_t - y_x)))
            mape_x = float(np.mean(np.abs((y_t - y_x) / denom)) * 100.0)
            wmape_x = float(np.sum(np.abs(y_t - y_x)) / max(tail_sum, 1.0) * 100.0)
            dfw_x = pd.DataFrame({"date": df_p["date"].values, "y_true": y_t, "y_pred": y_x})
            gw_x = dfw_x.set_index("date").groupby(pd.Grouper(freq="W")).sum().reset_index()
            denom_w_x = np.where(gw_x["y_true"].values == 0, 1, gw_x["y_true"].values)
            weekly_mape_x = float(
                np.mean(np.abs((gw_x["y_true"].values - gw_x["y_pred"].values) / denom_w_x)) * 100.0
            )
            row.update(
                {
                    "XGB_MAE": round(mae_x, 2),
                    "XGB_MAPE": round(mape_x, 2),
                    "GAIN_XGB_vs_LGBM_MAE": round(mae_l - mae_x, 2),
                    "GAIN_XGB_vs_LGBM_MAPE": round(mape_l - mape_x, 2),
                    "XGB_wMAPE": round(wmape_x, 2),
                    "XGB_wkMAPE": round(weekly_mape_x, 2),
                }
            )
        if y_xps is not None:
            mae_xps = float(np.mean(np.abs(y_t - y_xps)))
            mape_xps = float(np.mean(np.abs((y_t - y_xps) / denom)) * 100.0)
            wmape_xps = float(np.sum(np.abs(y_t - y_xps)) / max(tail_sum, 1.0) * 100.0)
            dfw_xps = pd.DataFrame({"date": df_p["date"].values, "y_true": y_t, "y_pred": y_xps})
            gw_xps = dfw_xps.set_index("date").groupby(pd.Grouper(freq="W")).sum().reset_index()
            denom_w_xps = np.where(gw_xps["y_true"].values == 0, 1, gw_xps["y_true"].values)
            weekly_mape_xps = float(
                np.mean(np.abs((gw_xps["y_true"].values - gw_xps["y_pred"].values) / denom_w_xps))
                * 100.0
            )
            row.update(
                {
                    "XGBps_MAE": round(mae_xps, 2),
                    "XGBps_MAPE": round(mape_xps, 2),
                    "GAIN_XGBps_vs_LGBM_MAE": round(mae_l - mae_xps, 2),
                    "GAIN_XGBps_vs_LGBM_MAPE": round(mape_l - mape_xps, 2),
                    "XGBps_wMAPE": round(wmape_xps, 2),
                    "XGBps_wkMAPE": round(weekly_mape_xps, 2),
                }
            )
        if y_rf_pair is not None:
            mae_rf_pair = float(np.mean(np.abs(y_t - y_rf_pair)))
            mape_rf_pair = float(np.mean(np.abs((y_t - y_rf_pair) / denom)) * 100.0)
            wmape_rf = float(np.sum(np.abs(y_t - y_rf_pair)) / max(tail_sum, 1.0) * 100.0)
            dfw_rf = pd.DataFrame({"date": df_p["date"].values, "y_true": y_t, "y_pred": y_rf_pair})
            gw_rf = dfw_rf.set_index("date").groupby(pd.Grouper(freq="W")).sum().reset_index()
            denom_w_rf = np.where(gw_rf["y_true"].values == 0, 1, gw_rf["y_true"].values)
            weekly_mape_rf = float(
                np.mean(np.abs((gw_rf["y_true"].values - gw_rf["y_pred"].values) / denom_w_rf))
                * 100.0
            )
            row.update(
                {
                    "RF_MAE": round(mae_rf_pair, 2),
                    "RF_MAPE": round(mape_rf_pair, 2),
                    "GAIN_RF_vs_LGBM_MAE": round(mae_l - mae_rf_pair, 2),
                    "GAIN_RF_vs_LGBM_MAPE": round(mape_l - mape_rf_pair, 2),
                    "RF_wMAPE": round(wmape_rf, 2),
                    "RF_wkMAPE": round(weekly_mape_rf, 2),
                }
            )
        rows.append(row)
    except Exception:
        continue

if rows:
    dfm = pd.DataFrame(rows)
    # –°–≤–æ–¥–∫–∞ (–ø–æ –º–∞–≥–∞–∑–∏–Ω–∞–º/—Å–µ–º–µ–π—Å—Ç–≤–∞–º) —Å wMAPE –∏ Weekly MAPE + —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –≤—ã–≥—Ä—É–∑–∫–∏
    st.subheader("–°–≤–æ–¥–∫–∞: –∞–≥—Ä–µ–≥–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞")
    cols = [
        c for c in dfm.columns if c.endswith(("_MAE", "_MAPE")) or ("wMAPE" in c) or ("wkMAPE" in c)
    ]
    if cols:
        group_mode = st.radio("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–∫—É", ["–ü–æ –º–∞–≥–∞–∑–∏–Ω–∞–º", "–ü–æ —Å–µ–º—å—è–º"], horizontal=True)
        group_col = "store_nbr" if group_mode == "–ü–æ –º–∞–≥–∞–∑–∏–Ω–∞–º" else "family"
        summary = dfm.groupby(group_col)[cols].mean().reset_index()

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        sort_metric = st.selectbox(
            "–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –º–µ—Ç—Ä–∏–∫–µ", options=cols, index=0, key="sum_sort_metric"
        )
        sort_order = st.radio(
            "–ü–æ—Ä—è–¥–æ–∫",
            ["–ø–æ —É–±—ã–≤–∞–Ω–∏—é", "–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é"],
            horizontal=True,
            index=0,
            key="sum_sort_order",
        )
        asc = True if sort_order == "–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é" else False
        summary_sorted = summary.sort_values(sort_metric, ascending=asc)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏
        min_v = float(np.nanmin(summary_sorted[sort_metric].values))
        max_v = float(np.nanmax(summary_sorted[sort_metric].values))
        # —à–∞–≥ –ø–æ–¥–±–∏—Ä–∞–µ–º –≥—Ä—É–±–æ
        step = max((max_v - min_v) / 100.0, 1e-6)
        rng = st.slider(
            "–î–∏–∞–ø–∞–∑–æ–Ω –º–µ—Ç—Ä–∏–∫–∏",
            min_value=float(min_v),
            max_value=float(max_v),
            value=(float(min_v), float(max_v)),
            step=step,
            key="sum_range_slider",
        )
        summary_filtered = summary_sorted[
            (summary_sorted[sort_metric] >= rng[0]) & (summary_sorted[sort_metric] <= rng[1])
        ]

        # Top‚ÄëN –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        top_n = st.number_input(
            "–ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø‚ÄëN —Å—Ç—Ä–æ–∫",
            min_value=1,
            max_value=1000,
            value=min(50, len(summary_filtered)),
            step=1,
            key="sum_top_n",
        )
        summary_view = summary_filtered.head(int(top_n))
        summary_display = summary_view.copy()
        for col in cols:
            if col in summary_display.columns:
                summary_display[col] = summary_display[col].round(2)
        try:
            styled = summary_display.style.background_gradient(
                cmap="YlGnBu", subset=[c for c in cols if ("wMAPE" in c) or ("wkMAPE" in c)]
            )
            st.dataframe(styled, use_container_width=True)
        except Exception:
            st.dataframe(summary_display, use_container_width=True)
        # –í—ã–≥—Ä—É–∑–∫–∏
        st.download_button(
            f"‚¨áÔ∏è CSV: —Å–≤–æ–¥–∫–∞ ({'stores' if group_col=='store_nbr' else 'families'})",
            data=summary_display.to_csv(index=False).encode("utf-8"),
            file_name=(
                "summary_per_store_extended.csv"
                if group_col == "store_nbr"
                else "summary_per_family_extended.csv"
            ),
            mime="text/csv",
        )

    dim_col = "store_nbr" if dim_choice == "–ü–æ –º–∞–≥–∞–∑–∏–Ω–∞–º" else "family"
    # CatBoost vs LGBM
    if {"GAIN_CB_vs_LGBM_MAE", "GAIN_CB_vs_LGBM_MAPE"}.issubset(dfm.columns):
        val = "GAIN_CB_vs_LGBM_MAE" if metric_choice == "MAE" else "GAIN_CB_vs_LGBM_MAPE"
        pv = dfm.pivot_table(
            index=dim_col,
            columns="family" if dim_col == "store_nbr" else "store_nbr",
            values=val,
            aggfunc="mean",
        ).fillna(0.0)
        pv_display = pv.round(2)
        st.subheader("Heatmap: CatBoost vs LGBM (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = CatBoost –ª—É—á—à–µ)")
        st.dataframe(pv_display.style.background_gradient(cmap="RdYlGn"), use_container_width=True)
        st.download_button(
            "‚¨áÔ∏è CSV: heatmap CB vs LGBM",
            data=pv_display.to_csv().encode("utf-8"),
            file_name="heatmap_cb_vs_lgbm.csv",
            mime="text/csv",
        )
    # XGB vs LGBM
    if {"GAIN_XGB_vs_LGBM_MAE", "GAIN_XGB_vs_LGBM_MAPE"}.issubset(dfm.columns):
        val = "GAIN_XGB_vs_LGBM_MAE" if metric_choice == "MAE" else "GAIN_XGB_vs_LGBM_MAPE"
        pv = dfm.pivot_table(
            index=dim_col,
            columns="family" if dim_col == "store_nbr" else "store_nbr",
            values=val,
            aggfunc="mean",
        ).fillna(0.0)
        pv_display = pv.round(2)
        st.subheader("Heatmap: XGB (global) vs LGBM (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = XGB –ª—É—á—à–µ)")
        st.dataframe(pv_display.style.background_gradient(cmap="RdYlGn"), use_container_width=True)
        st.download_button(
            "‚¨áÔ∏è CSV: heatmap XGB vs LGBM",
            data=pv_display.to_csv().encode("utf-8"),
            file_name="heatmap_xgb_vs_lgbm.csv",
            mime="text/csv",
        )
    # XGB per‚ÄëSKU vs LGBM
    if {"GAIN_XGBps_vs_LGBM_MAE", "GAIN_XGBps_vs_LGBM_MAPE"}.issubset(dfm.columns):
        val = "GAIN_XGBps_vs_LGBM_MAE" if metric_choice == "MAE" else "GAIN_XGBps_vs_LGBM_MAPE"
        pv = dfm.pivot_table(
            index=dim_col,
            columns="family" if dim_col == "store_nbr" else "store_nbr",
            values=val,
            aggfunc="mean",
        ).fillna(0.0)
        pv_display = pv.round(2)
        st.subheader("Heatmap: XGB per‚ÄëSKU vs LGBM (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = XGB per‚ÄëSKU –ª—É—á—à–µ)")
        st.dataframe(pv_display.style.background_gradient(cmap="RdYlGn"), use_container_width=True)
        st.download_button(
            "‚¨áÔ∏è CSV: heatmap XGB per‚ÄëSKU vs LGBM",
            data=pv_display.to_csv().encode("utf-8"),
            file_name="heatmap_xgbps_vs_lgbm.csv",
            mime="text/csv",
        )
    if {"GAIN_RF_vs_LGBM_MAE", "GAIN_RF_vs_LGBM_MAPE"}.issubset(dfm.columns):
        val = "GAIN_RF_vs_LGBM_MAE" if metric_choice == "MAE" else "GAIN_RF_vs_LGBM_MAPE"
        pv = dfm.pivot_table(
            index=dim_col,
            columns="family" if dim_col == "store_nbr" else "store_nbr",
            values=val,
            aggfunc="mean",
        ).fillna(0.0)
        pv_display = pv.round(2)
        st.subheader("Heatmap: RandomForest vs LGBM (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ = RandomForest –ª—É—á—à–µ)")
        st.dataframe(pv_display.style.background_gradient(cmap="RdYlGn"), use_container_width=True)
        st.download_button(
            "‚¨áÔ∏è CSV: heatmap RF vs LGBM",
            data=pv_display.to_csv().encode("utf-8"),
            file_name="heatmap_rf_vs_lgbm.csv",
            mime="text/csv",
        )
else:
    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤–æ–¥–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã.")
